{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marking org.apache.kafka:kafka_2.11:0.10.0.1 for download\n",
      "Obtained 2 files\n",
      "Marking org.apache.kafka:kafka-clients:2.0.0 for download\n",
      "Obtained 2 files\n",
      "Marking org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.0 for download\n",
      "Obtained 2 files\n",
      "Marking org.apache.spark:spark-streaming-kafka-0-10_2.11:2.2.0 for download\n",
      "Obtained 2 files\n",
      "Marking org.apache.spark:spark-avro_2.11:2.4.0 for download\n",
      "Obtained 2 files\n",
      "Marking org.apache.hadoop:hadoop-aws:2.7.3 for download\n",
      "Obtained 2 files\n",
      "Marking com.amazonaws:aws-java-sdk:1.7.4 for download\n",
      "Obtained 2 files\n"
     ]
    }
   ],
   "source": [
    "%AddDeps org.apache.kafka kafka_2.11 0.10.0.1\n",
    "%AddDeps org.apache.kafka kafka-clients 2.0.0\n",
    "%AddDeps org.apache.spark spark-sql-kafka-0-10_2.11 2.4.0\n",
    "%AddDeps org.apache.spark spark-streaming-kafka-0-10_2.11 2.2.0\n",
    "%AddDeps org.apache.spark spark-avro_2.11 2.4.0\n",
    "%AddDeps org.apache.hadoop hadoop-aws 2.7.3\n",
    "%AddDeps com.amazonaws aws-java-sdk 1.7.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spark = org.apache.spark.sql.SparkSession@64f51e6d\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.SparkSession@64f51e6d"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " val spark = SparkSession\n",
    "    .builder()\n",
    "    .appName(\"Events Gateway Consumer\")\n",
    "    // .config(\"spark.jars.packages\", \"com.amazonaws:aws-java-sdk:1.10.8,org.apache.hadoop:hadoop-aws:2.9.2\")\n",
    "    // .config(\"spark.driver.extraClassPath\", \"/tmp/toree-tmp-dir6121719992159655462/toree_add_deps/https/repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/2.9.2/hadoop-aws-2.9.2.jar:/tmp/toree-tmp-dir6121719992159655462/toree_add_deps/https/repo1.maven.org/maven2/com/amazonaws/aws-java-sdk/1.10.8/aws-java-sdk-1.10.8.jar\")\n",
    "    // .config(\"spark.executor.extraClassPath\", \"/tmp/toree-tmp-dir6121719992159655462/toree_add_deps/https/repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/2.9.2/hadoop-aws-2.9.2.jar:/tmp/toree-tmp-dir6121719992159655462/toree_add_deps/https/repo1.maven.org/maven2/com/amazonaws/aws-java-sdk/1.10.8/aws-java-sdk-1.10.8.jar\")\n",
    "    // .config(\"hive.metastore.warehouse.dir\", hiveWarehouse)\n",
    "    // .config(\"fs.defaultFS\", hdfs_FS)\n",
    "    // .enableHiveSupport()\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.avro._\n",
    "import java.nio.file.{Files, Paths}\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.Row\n",
    "import org.apache.spark.sql.functions._\n",
    "import scala.collection.mutable.WrappedArray\n",
    "import scala.collection.immutable.Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "kafka_topic = sv-uploads-default-topic\n",
       "kafka_brokers = kafka:9092\n",
       "kafka_from = earliest\n",
       "kafka_maxoffsets = 10000\n",
       "kafka_maxfiles = 1\n",
       "s3_bucket = eventsgateway-local\n",
       "environment = development\n",
       "jsonFormatSchema = \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{\n",
       "  \"namespace\": \"com.tfgco.eventsgateway\",\n",
       "  \"type\": \"record\",\n",
       "  \"name\": \"Event\",\n",
       "  \"fields\": [\n",
       "    {\n",
       "      \"name\": \"id\",\n",
       "      \"type\": \"string\"\n",
       "    },\n",
       "    {\n",
       "      \"name\": \"name\",\n",
       "      \"type\": \"string\"\n",
       "    },\n",
       "    {\n",
       "      \"name\": \"props\",\n",
       "      \"default\": {},\n",
       "      \"type\": {\n",
       "        \"type\": \"map\",\n",
       "        \"values\": \"string\"\n",
       "      }\n",
       "    },\n",
       "    {\n",
       "      \"name\": \"serverTimestamp\",\n",
       "      \"type\": \"long\"\n",
       "    },\n",
       "    {\n",
       "      \"name\": \"clientTimestamp\",\n",
       "      \"type\": \"long\"\n",
       "    }\n",
       "  ]\n",
       "}\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val kafka_topic = \"sv-uploads-default-topic\"\n",
    "if (kafka_topic == \"\") {\n",
    "      throw new IllegalStateException(\"topic variable is empty!\");\n",
    "}\n",
    "\n",
    "val kafka_brokers = \"kafka:9092\"\n",
    "val kafka_from = \"earliest\"\n",
    "val kafka_maxoffsets = \"10000\"\n",
    "val kafka_maxfiles = \"1\"\n",
    "val s3_bucket = \"eventsgateway-local\"\n",
    "\n",
    "val jsonFormatSchema : String = \"\"\"{\n",
    "  \"namespace\": \"com.tfgco.eventsgateway\",\n",
    "  \"type\": \"record\",\n",
    "  \"name\": \"Event\",\n",
    "  \"fields\": [\n",
    "    {\n",
    "      \"name\": \"id\",\n",
    "      \"type\": \"string\"\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"name\",\n",
    "      \"type\": \"string\"\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"props\",\n",
    "      \"default\": {},\n",
    "      \"type\": {\n",
    "        \"type\": \"map\",\n",
    "        \"values\": \"string\"\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"serverTimestamp\",\n",
    "      \"type\": \"long\"\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"clientTimestamp\",\n",
    "      \"type\": \"long\"\n",
    "    }\n",
    "  ]\n",
    "}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "input_df = [key: binary, value: binary ... 5 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[key: binary, value: binary ... 5 more fields]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val input_df = spark\n",
    "  .readStream\n",
    "  .format(\"kafka\")\n",
    "  .option(\"maxFilesPerTrigger\", kafka_maxfiles)\n",
    "  .option(\"kafka.bootstrap.servers\", kafka_brokers)\n",
    "  .option(\"subscribe\", kafka_topic)\n",
    "  .option(\"startingOffsets\", kafka_from)\n",
    "  .option(\"maxOffsetsPerTrigger\", kafka_maxoffsets)\n",
    "  .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df = [id: string, name: string ... 6 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[id: string, name: string ... 6 more fields]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = input_df\n",
    "    .withColumn(\"event\", from_avro($\"value\", jsonFormatSchema))\n",
    "    .withColumn(\"id\", $\"event.id\")\n",
    "    .withColumn(\"name\", $\"event.name\")\n",
    "    .withColumn(\"props\", $\"event.props\")\n",
    "    .withColumn(\"clienttimestamp\", $\"event.clientTimestamp\")\n",
    "    .withColumn(\"servertimestamp\", $\"event.serverTimestamp\")\n",
    "    .withColumn(\"date\", to_date(from_unixtime($\"event.clientTimestamp\" / 1000)))\n",
    "    .withColumn(\"year\", date_format($\"date\", \"YYYY\"))\n",
    "    .withColumn(\"month\", date_format($\"date\", \"MM\"))\n",
    "    .withColumn(\"day\", date_format($\"date\", \"dd\"))\n",
    "    .select(\"id\", \"name\", \"props\", \"clienttimestamp\", \"servertimestamp\", \"year\", \"month\", \"day\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hadoop_conf = Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml, __spark_hadoop_conf__.xml\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml, __spark_hadoop_conf__.xml"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val hadoop_conf = spark.sparkContext.hadoopConfiguration\n",
    "hadoop_conf.set(\"fs.s3.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "// hadoop_conf.set(\"fs.s3a.endpoint\", \"http://localstack:4572/eventsgateway-local\")\n",
    "// hadoop_conf.set(\"fs.s3a.path.style.access\", \"true\")\n",
    "hadoop_conf.set(\"fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "hadoop_conf.set(\"fs.s3a.access.key\", \"dummy\")\n",
    "hadoop_conf.set(\"fs.s3a.secret.key\", \"dummy\")\n",
    "hadoop_conf.set(\"fs.s3a.proxy.host\", \"eventsgatewaylocalproxy\")\n",
    "hadoop_conf.set(\"fs.s3a.proxy.port\", \"80\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "query = org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@704f7333\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@704f7333"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val query = df\n",
    "    .writeStream\n",
    "    .format(\"orc\")\n",
    "    .outputMode(\"append\")\n",
    "    .option(\"path\", s\"s3a://$s3_bucket/var/standard/$kafka_topic/data/\")\n",
    "    .option(\"checkpointLocation\", \"/tmp/checkpoints\")\n",
    "    // .option(\"checkpointLocation\", s\"s3a://$s3_bucket/var/standard/checkpoints/$kafka_topic/\") // databricks only\n",
    "    .partitionBy(\"year\", \"month\", \"day\")\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATE DATABASE IF NOT EXISTS eventsgateway_local LOCATION 's3a://eventsgateway-local/var/standard/databases/' \n",
      "\n",
      "CREATE EXTERNAL TABLE IF NOT EXISTS `eventsgateway_local.sv_uploads_default_topic` (`id` string, `name` string, `props` map<string,string>, `servertimestamp` bigint, `clienttimestamp` bigint) PARTITIONED BY (`year` STRING, `month` STRING, `day` STRING) STORED AS ORC LOCATION 's3a://eventsgateway-local/var/standard/sv-uploads-default-topic/data/';\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dbname = eventsgateway_local\n",
       "tablename = sv_uploads_default_topic\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "sv_uploads_default_topic"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dbname = s3_bucket.replace('-', '_')\n",
    "val tablename = kafka_topic.replace('-', '_')\n",
    "\n",
    "// RUN THIS INSIDE HIVE ITSELF THROUGH `ocker exec -it hive_hive-server_1 sh -c \"/opt/hive/bin/beeline -u jdbc:hive2://localhost:10000\"`\n",
    "\n",
    "println(s\"\"\"CREATE DATABASE IF NOT EXISTS $dbname LOCATION 's3a://$s3_bucket/var/standard/databases/' \"\"\")\n",
    "println()\n",
    "println(s\"\"\"CREATE EXTERNAL TABLE IF NOT EXISTS `$dbname.$tablename` (`id` string, `name` string, `props` map<string,string>, `servertimestamp` bigint, `clienttimestamp` bigint) PARTITIONED BY (`year` STRING, `month` STRING, `day` STRING) STORED AS ORC LOCATION 's3a://$s3_bucket/var/standard/$kafka_topic/data/';\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "customDeps": [
   "com.databricks:spark-avro_2.10:4.0.0"
  ],
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
